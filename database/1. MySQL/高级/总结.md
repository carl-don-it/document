# [MySQL 实战 45 讲](https://time.geekbang.org/column/intro/100020801) 

看一个事儿千万不要直接陷入细节里，你应该先鸟瞰其全貌，这样能够帮助你从高维度理解问题。

# 01讲基础架构：一条SQL查询语句是如何执行的

## MySQL的逻辑架构图

## 连接器

建立连接、获取权限、维持和管理连接。

修改完成后，只有再新建的连接才会使用新的权限设置。

wait_timeout

## 长连接

尽量使用长连接。

MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。

定期断开长连接。

mysql_reset_connection。

## 查询缓存

**但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。**

查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空

## 分析器

词法分析

语法分析

语义解析

  解析器处理语法和解析查询, 生成一课对应的解析树。

预处理器进一步检查解析树的合法。比如: 数据表和数据列是否存在, 别名是否有歧义等。如果通过则生成新的解析树，再提交给优化器。  

## 优化器

执行计划

## 执行器

对这个表T有没有执行查询的权限

使用这个引擎提供的接口

rows_examined。在执行器每次调用引擎获取数据行的时候累加的。Rows_examined 是server层统计的，这个不满足的值没返回给server



# 02讲日志系统：一条SQL更新语句是如何执行的



## 重要的日志模块：redo log

在MySQL里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。

MySQL里经常说到的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，

write pos是当前记录的位置

checkpoint是当前要擦除的位置

保证crash-safe能力

innodb_flush_log_at_trx_commit

## 重要的日志模块：binlog



1. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。



update语句时的内部流程。

执行器，引擎

prepare和commit，这就是"两阶段提交"。



## 两阶段提交

**怎样让数据库恢复到半个月内任意一秒的状态？**

## 一天一备跟一周一备的对比

# 03讲事务隔离：为什么你改了我还看不见



## 隔离性与隔离级别

在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。

**什么时候需要“可重复读”的场景呢**？

## 事务隔离的实现

当系统里没有比这个回滚日志更早的read-view的时候。回滚日志会被删除。

尽量不要使用长事务

除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库

## 事务的启动方式

建议你总是使用set autocommit=1, 通过显式语句的方式来启动事务。

 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销

查询长事务

```mysql
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

## 如何用正确的方式避免长事务

SET MAX_EXECUTION_TIME

# 04讲深入浅出索引（上）

## 索引的常见模型

hash array tree 

## InnoDB 的索引模型

索引组织表

**主键索引和普通索引**

回表

## 索引维护

页分裂 合并

**主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。**所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。

“尽量使用主键查询”原则.

二级索引重建应该新建索引再做删除。

表文件空间打， InnoDB 这种引擎导致的,虽然删除了表的部分记录,但是它的索引还在, 并未释放.只能是重新建表才能重建索引.优化表。

## 重建索引的作法

通过两个alter 语句重建索引k，以及通过两个alter语句重建主键索引是否合理。

drop主键索引会导致其他索引失效.

# 05讲深入浅出索引（下）

## SQL查询语句的执行流程

执行计划

## 覆盖索引



**在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？**

如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了

## 最左前缀原则

这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。

## 索引下推

而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。



# 06讲全局锁和表锁：给表加个字段怎么有这么多阻碍

https://time.geekbang.org/column/article/69862

## 全局锁

全局读锁  Flush tables with read lock (FTWRL)

之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

**全局锁的典型使用场景是，做全库逻辑备份。**也就是把整库每个表都select出来存成文本。

不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。

mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图

**single-transaction方法只适用于所有的表使用事务引擎的库。**

FTWRL 前有读写的话 ,FTWRL 都会等待 读写执行完毕后才执行
FTWRL 执行的时候要刷脏页的数据到磁盘,因为要保持数据的一致性 ，理解的执行FTWRL时候是 所有事务 都提交完毕的时候

## 表级锁

### 表锁

**lock tables … read/write**

lock tables t1 read, t2 write;

与FTWRL类似，可以用unlock tables主动释放锁。

也可以在客户端断开的时候自动释放。

### 元数据锁

MDL不需要显式使用，在访问一个表的时候会被自动加上。

在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

MDL 是并发情况下维护数据的一致性,在表上有事务的时候,不可以对元数据经行写入操作,并且这个是在server层面实现的。

当你DDL 时候 增加对表的写锁, 同时操作两个alter table 操作 这个要出现等待情况。

给一个小表加个字段，导致整个库挂了。长事务，锁是优先平均需要排队而不是优先读会饥饿的那种，其实版本不同实现不同

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session再请求的话，这个库的线程很快就会爆满。

## online ddl

## **如何安全地给小表加字段？**

## single-transaction备份加锁

备份一般都会在备库上执行，你在用–single-transaction方法做逻辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从备库上会看	到什么现象呢？

> 有个小问题，这样岂不是会得不到同一时刻下的数据库。
>
> 应该一开始加个锁，一次性获取所有的表结构才行。

# 07讲行锁功过：怎么减少行锁对性能的影响

https://time.geekbang.org/column/article/70215

MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。

## 从两阶段锁说起

**在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。**

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

## 死锁和死锁检测

死锁检测要耗费大量的CPU资源。

**临时把死锁检测关掉**

**控制并发度** 并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现

将一行改成逻辑上的多行来减少锁冲突。(分段锁)



## 删除一个表里面的前10000行数据

有以下三种方法可以做到：

这个要看数据库压力，如果数据库非常空闲，我选方案一，这样操作简单。如果数据库中这张表的压力非常大，我选方案三，极端情况下甚至我会制定方案四，每次只删一条。

# 08讲事务到底是隔离的还是不隔离的

https://time.geekbang.org/column/article/70562

## 视图

- 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view ... ，而它的查询方法与表一样。
- 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现

## “快照”在MVCC里是怎么工作的？

 InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。

数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

**代码实现上，获取视图数组和高水位是在事务系统的锁保护下做的，可以认为是原子操作，期间不能创建事务。**

所以，我来给你翻译一下。一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：

1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。

## 更新逻辑

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**

select语句如果加锁，也是当前读。

## **事务的可重复读的能力是怎么实现的**

- 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。
- “start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的start transaction。

## 更新丢失

并发更新，没有加锁导致

查询不需要在一个事务中，一般也用不上可重复读的，而且可重复读还会造成读不到最新的记录（而这正是读已提交所提供的）。看业务需求

一般查询在一个事务中，要么是需要可重复读，要么是加锁。

# 09讲普通索引和唯一索引，应该怎么选择

**咱们这篇文章的前提是“业务代码已经保证不会写入重复数据”的情况下，讨论性能问题。**

**如果碰上了大量插入数据慢、内存命中率低的时候，可以给你多提供一个排查思路。**

**然后，在一些“归档库”的场景，你是可以考虑使用唯一索引的。**

由于身份证号字段比较大，我不建议你把身份证号当做主键，那么现在你有两个选择，要么给id_card字段创建唯一索引，要么创建一个普通索引。如果业务代码已经保证了不会写入重复的身份证号，那么这两个选择逻辑上都是正确的。

现在我要问你的是，从性能的角度考虑，你选择唯一索引还是普通索引呢？选择的依据是什么呢

## 查询过程

几乎没有区别

## 更新过程

**change buffer** 

只有普通索引可以使用

将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。

减少读磁盘，避免占用内存，提高内存利用率。

## change buffer的使用场景

使用change buffer都可以起到加速作用吗？



因为merge的时候是真正进行数据更新的时刻，而change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。



账单类、日志类

~~写入之后马上会做查询~~

## 索引选择和实践

普通索引和change buffer的配合使用，对于数据量大的表的更新优化还是很明显的。特别地，在使用机械硬盘时

## change buffer 和 redo log

描写的都是辅助索引树上的操作，并不是聚簇索引上的操作。

redo log也保存change buffer 中的操作

**Ivan的回答**

change buffer 另外用一棵b树实现，写入到了系统表空间，至于为什么要新开一棵树而不是直接用redolog记录，可能是为了性能。

## 讨论

change buffer的实现，redolog的配合，系统表空间

## merge的过程

下一张

# 10讲MySQL为什么有时候会选错索引

平常不断地删除历史数据和新增数据的场景。这时，MySQL竟然会选错索引



## 优化器的逻辑

选择索引是优化器的工作。

当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。回表

explain rows这个字段表示的是预计扫描行数。

analyze table t 

## 索引选择异常和处理

**force index**

**考虑修改语句，引导MySQL使用我们期望的索引**

order by b,a

> 其实排序量也很小吧，不了解为什么要选a



**在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。**

## 删除数据的过程

# 11讲怎么给字符串字段加索引

MySQL是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。



## 完整索引和前缀索引

使用前缀索引后，可能会导致查询语句读数据的次数变多。

如何选择前缀索引的长度

空间和查询成本

建立索引时关注的是区分度，区分度越高越好。

```mysql
select count(distinct email) as L from SUser;
select 
  count(distinct left(email,4)）as L4,
  count(distinct left(email,5)）as L5,
  count(distinct left(email,6)）as L6,
  count(distinct left(email,7)）as L7,
from SUser;
```

## 前缀索引对覆盖索引的影响

使用前缀索引就用不上覆盖索引对查询性能的优化

## 其他方式

等值查询

前缀的区分度不够好

### **倒序存储**

### **hash**

crc32()

从业务量预估优化和收益

# 12讲为什么我的MySQ会“抖”一下

## flush脏页 checkpoints

## 你的SQL语句为什么变“慢”了

redo log写满了，要flush脏页，更新数会跌为0

一个查询要淘汰的脏页个数太多

## InnoDB刷脏页的控制策略

主机的IO能力

redo log写盘速度。

脏页比例

**平时要多关注脏页比例，不要让它经常接近75%**

innodb_flush_neighbors

## redo log设置太小，会发生什么情况？

redo log在“重放”的时候，如果一个数据页已经是刷过的，会识别出来并跳过。

# 13讲为什么表数据删掉一半，表文件大小不变

https://time.geekbang.org/column/article/72388

表的空间回收，InnoDB引擎

MySQL 8.0版本以前，表结构是存在以.frm为后缀的文件里。而MySQL 8.0版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。

## 参数innodb_file_per_table

如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。

删除整个表的时候，可以使用drop table命令回收表空间。

## 数据删除流程 空洞

标记为删除

复用位置

delete命令其实只是把记录的位置，或者数据页标记为了“可复用”

插入分裂**空洞**

更新=删除+插入

## 重建表

alter table A engine=InnoDB，从MySQL 5.6版本开始

**Online DDL**

1. 拿MDL写锁
2. 降级成MDL读锁
3. 真正做DDL
4. 升级成MDL写锁
5. 释放MDL锁

日志文件记录和重放操作

消耗IO和CPU

在重建表的时候，InnoDB不会把整张表占满，每个页留了1/16给后续的更新用。也就是说，其实重建表之后不是“最”紧凑的。

## Online 和 inplace

inplace 数据没有经过server层，都是在引擎内部

ALGORITHM=copy

DDL过程如果是Online的，就一定是inplace的；反过来未必

不影响增删改，就是 Online；相对 Server层没有新建临时表，就是 inplace

添加全文索引（FULLTEXT index）和空间索引(SPATIAL index)



## optimize table、analyze table和alter table、Truncate 



Truncate 可以理解为drop+create



重新收缩的过程中，页会按90%满的比例来重新整理页数据（10%留给UPDATE使用），





怎么判断是不是相对 Server 层没有新建临时表。一个最直观的判断方法是看命令执行后影响的行数，没有新建临时表的话新建的行数是0。



show table status like 't';



## MySQL各版本，对于add Index的处理方式



不是重建表，只是加辅助索引

# 14讲count(*)这么慢，我该怎么办

https://time.geekbang.org/column/article/72775

## count(*)的实现方式

MyISAM引擎把一个表的总行数存在了磁盘上

InnoDB引擎慢慢来

由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定

MySQL优化器会找到最小的那棵树来遍历

TABLE_ROWS 估算

## 用缓存系统保存计数

宕机

丢失更新

**逻辑上不精确**

## 在数据库保存计数

逻辑上就是一致

利用事务

## 不同的count用法

count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。

count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数。

count(字段)<count(主键id)<count(1)≈count(*)



没主键的表的从库重放，row 格式，会扫全表，bug，最好不用没主键的表

# 15讲答疑文章（一）：日志和索引相关问题

## 日志相关问题

**在两阶段提交的不同时刻，MySQL异常重启会出现什么现象。**

几大问题



## 业务设计问题

# 修改的值跟原来的值是相同

当MySQL去更新一行，但是要修改的值跟原来的值是相同的，这时候MySQL会真的去执行一次修改吗？还是看到值相同就直接返回呢？

update返回的是修改的行数

可以通过查看ibd文件和redo log文件的修改时间都更新了。通过show engine innodb status 进一步验证，查看LSN确实增加了，而且Number of rows updated 也加+1了。

有可能改了，也有可能没有改，有很多决定因素。所以很难判断。

改没改其实在应用上影响不大。即便是rr，也看不出有什么业务影响。

server层面和引擎层面有没有改可以去看文件有没有改变。



# 16讲“orderby”是怎么工作的

https://time.geekbang.org/column/article/73479

## sort_buffer

决定是否使用临时文件

> Extra这个字段中的“Using filesort”表示的就是需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。
>
> sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。在使用外部排序时，MySQL会分成好几份单独的临时文件用来存放排序后的数据，然后在将这些文件合并成一个大文件
>

## max_length_for_sort_data

决定使用全字段排序和rowid排序。

max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。

## 全字段排序

全字段排序流程。

是否使用了临时文件

> OPTIMIZER_TRACE

## rowid排序

rowid排序过程，放入sort_buffer的字段，只有要排序的列（即name字段）和主键id

结果集不需要在服务端再耗费内存存储结果，是直接返回给客户端的。

## 全字段排序 VS rowid排序

对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

排序不一定需要临时表

## 原数据有序

order by 索引

查询流程

## 用业务代码优化排序

用其他思想，分开处理



# 17讲如何正确地显示随机消息

https://time.geekbang.org/column/article/73795

## 内存临时表

Using temporary

Using filesort

> Extra字段显示Using temporary，表示的是需要使用临时表（有可能memory内存临时表，有可能innodb硬盘临时表，看大小）；Using filesort，表示的是需要执行排序操作。
>
> 因此这个Extra的意思就是，需要临时表，并且需要在临时表上排序。

临时表是什么

order by rand()的排序过程

> **对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘**。MySQL这时就会选择rowid排序。

理论分析扫描行数，慢查询验证结论

rowid名字的来历

## 磁盘临时表

什么时候用磁盘临时表

> tmp_table_size限制内存临时表的大小
>
> 磁盘临时表使用的引擎默认是InnoDB   internal_tmp_disk_storage_engine

OPTIMIZER_TRACE 

## 排序算法

归并排序算法

> 所有应该排列的数据都有序

优先队列排序算法

> MySQL 5.6版本引入
>
> limit个数据有序就可以了，大堆顶，小堆顶
>
> filesort_priority_queue_optimization
>
> number_of_tmp_files=0
>
> 数据量不能超过sort_buffer_size，使用归并排序算法。

## 巧妙处理随机排序方法

核心就是尽量不排序

### 随机id

> 如果ID中间有空洞，选择不同行的概率不一样，不是真正的随机。
>
> 可以重新整理表处理空洞

### 随机第几行

> 需要先求总行数
>
> 扫描行数也不少
>
> 多个随机行，可用id进一步优化减少扫描行数

### 直接使用缓存

不用数据库

## limit的处理

MySQL处理limit Y,1 的做法就是按顺序一个一个地读出来，丢掉前Y个，然后把下一个记录作为返回结果，因此这一步需要扫描Y+1行。

# 18讲为什么这些SQL语句逻辑相同，性能却差异巨大

https://time.geekbang.org/column/article/74059

对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**

## 案例一：条件字段函数操作

```
mysql> select count(*) from tradelog where month(t_modified)=7;
```

## 案例二：隐式类型转换

在MySQL中，字符串和数字做比较的话，是将字符串转换成数字。

```
mysql> select * from tradelog where  CAST(tradid AS signed int) = 110717;
```

如果id 的类型是整数，传入的参数类型是字符串的时候，可以用上索引。

## 案例三：隐式字符编码转换

(如果驱动表的字符集比被驱动表得字符集小，关联列就能用到索引,如果更大,需要发生隐式编码转换,则不能用到索引,latin<gbk<utf8<utf8mb4)

```
select * from trade_detail  where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; 
```

对于select * from tradelog where id + 1 = 10000这个SQL语句，这个加1操作并不会改变有序性，但是MySQL优化器还是不能用id索引快速定位到9999这一行。所以，需要你在写SQL语句的时候，手动改写成 where id = 10000 -1才可以。

## 其他

  二.嵌套循环,驱动表与被驱动表选择错误
1.连接列上没有索引,导致大表驱动小表,或者小表驱动大表(但是大表走的是全表扫描) --连接列上建立索引
2.连接列上虽然有索引,但是驱动表任然选择错误。--通过straight_join强制选择关联表顺序
3.子查询导致先执行外表在执行子查询,也是驱动表与被驱动表选择错误。
--可以考虑把子查询改写为内连接,或者改写内联视图(子查询放在from后组成一个临时表,在于其他表进行关联)
4.只需要内连接的语句,但是写成了左连接或者右连接。比如select * from t left join b on t.id=b.id where b.name='abc'驱动表被固定,大概率会扫描更多的行,导致效率降低.
--根据业务情况或sql情况,把左连接或者右连接改写为内连接

三.索引选择不同,造成性能差异较大
1.select * from t where aid= and create_name>'' order by id limit 1;
选择走id索引或者选择走(aid,create_time)索引,性能差异较大.结果集都有可能不一致
--这个可以通过where条件过滤的值多少来大概判断,该走哪个索引

四.其它一些因素
1.比如之前学习到的是否有MDL X锁
2.innodb_buffer_pool设置得太小,innodb_io_capacity设置得太小,刷脏速度跟不上
3.是否是对表做了DML语句之后,马上做select,导致change buffer收益不高
4.是否有数据空洞
5.select选取的数据是否在buffer_pool中
6.硬件原因,资源抢占
原因多种多样,还需要慢慢补充。  

# 19讲为什么我只查一行的语句，也执行这么慢

https://time.geekbang.org/column/article/74687

被锁住和执行慢，涉及到了表锁、行锁和一致性读的概念。

一般碰到这种情况的话，大概率是被锁住了。

## 第一类：查询长时间不返回

### 等MDL锁

如何查询谁等待mdl锁，谁持有mdl锁

show processlist

Waiting for table metadata lock

performance_schema和sys系统库，sys.schema_table_lock_waits

> MySQL启动时需要设置performance_schema=on，相比于设置为off会有10%左右的性能损失

### 等flush

Waiting for table flush

### 等行锁

查询行锁信息

## 第二类：查询慢

### 不走索引，全表扫描

### rr、一致性读长事务，热点行，回滚日志多

### 截断字符串回表



## 文末的问题

某人的问题（邹涛的问题）	

rc和rr下的加锁







# 20讲幻读是什么，幻读有什么问题

## 幻读是什么？

幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

幻读在“当前读”下才会出现。

幻读仅专指“新插入的行”。

## 幻读有什么问题？

**锁的语义被破坏了**

> 没有锁住新的记录
>
> 更新带来的问题不是幻读导致的，而是只加行锁带来的问题

**数据一致性的问题。**

> binlog是串行的，和事务的并发相违背。主从不一致，备份恢复也有问题。

## 所有记录加锁

解决并发修改带来的主从数据不一致性，但是不能解决幻读、

## 如何解决幻读？

间隙锁

**跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。**间隙锁之间都不存在冲突关系。

间隙锁和行锁合称next-key lock

不存在的最大值suprenum

## 间隙锁的缺点

rr先锁后插入容易死锁，rc会报replicate key

**导致同样的语句锁住更大的范围，这其实是影响了并发度**

## 为什么要用读提交隔离级别加binlog_format=row

业务场景

备份线程用的是可重复读，而业务线程用的是读提交。同时存在两种事务隔离级别，会不会有问题？



# 21加锁规则

## rr

两个前提，**两个“原则”、两个“优化”和一个“bug”。**

> 其实很多锁法都是特定的，因为是具体mysql版本源码实现上的问题，知道大概锁法就好，（粗细粒度）偏差不影响业务，毕竟mysql不给出锁的规则就是说明数据库是动态的，并发的，锁随时会变。

没有记录时怎么锁

覆盖索引主键不加锁

**在删除数据的时候尽量加limit**。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。

死锁的例子

next-key lock要分成间隙锁和行锁两段来执行。

rr遵守两阶段锁协议

## rc

过程中去掉间隙锁的部分，也就是只剩下行锁的部分

语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交。

半一致性读

在read-commited隔离级别下，update语句有一个“semi-consistent” read优化，
意思是，如果update语句碰到一个已经被锁了的行，会读入最新的版本，然后判断一下是不是满足查询条件，
a)如果不满足，就直接跳过；
b) 如果满足，才进入锁等待

这个策略，只对update有效，delete无效。

这样做的好处是：减少同一行记录的锁冲突及锁等待；无并发冲突时，直接读取最新版本加锁，有冲突时，不加锁，读取prev版本不需要锁等待。

缺点：非冲突串行话策略，对于binlog来说是不安全的。只能发生在RC隔离级别和innodb_lock_unsafe_for_binlog下。（rc本就不安全，需要用row based复制）

- innodb_locks_unsafe_for_binlog 参数在 8.0 版本中已被去除（可见，这是一个可能会导致数据不一致的参数，官方也不建议使用了）。

# 22讲MySQL有哪些“饮鸩止渴”提高性能的方法

https://time.geekbang.org/column/article/75746

业务高峰期，生产环境的MySQL压力太大，没法正常响应，需要短期内、临时性地提升一些性能

## 短连接风暴

max_connections

**处理掉那些占着连接但是不工作的线程。**

客户端“ERROR 2013 (HY000): Lost connection to MySQL server during query

**减少连接过程的消耗。**

跳过权限验证 –skip-grant-tables  --skip-networking



## 慢查询性能问题

**索引没有设计好**

> Online DDL 
>
> 备库先执行

**SQL语句没写好**

> query_rewrite

**MySQL选错了索引**

**提前测试**

## QPS突增问题

下线功能，白名单，账号，改写

## 评论区案例

大事务

硬盘空间

崩溃恢复

表锁

高频事务，慢SQL

# 23讲MySQL是怎么保证数据不丢的

https://time.geekbang.org/column/article/76161

只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复。

redolog和binlog的机制和配置

## binlog的写入机制

binlog_cache_size

sync_binlog

出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。

## redo log的写入机制

redo log buffer

innodb_flush_log_at_trx_commit

## 组提交

基本的组提交，基于并发，单线程不起效

edo log延迟fsync优化

binlog也可以组提交

binlog组提交的参数，影响的是binlog的fsync

## 优化

基于redolog和binlog提高io速度

各个参数之间的关系

> sync_binlog = N
> binlog_group_commit_sync_no_delay_count = M
> binlog_group_commit_sync_delay =

## 非双一

**在什么时候会把线上生产库设置成“非双1”**

一般都是为了提高性能的时候

有调到非双1的时候,在大促时非核心库和从库延迟较多的情况。
设置的是sync_binlog=0和innodb_flush_log_at_trx_commit=2
针对0和2,在mysql crash时不会出现异常,在主机挂了时，会有几种风险:

# 24讲MySQL是怎么保证主备一致的

本章讲复制binlog

https://time.geekbang.org/column/article/76446g

## MySQL主备的基本原理

备库设置成只读的原因

一主一从，主从切换

binlog复制的流程

## binlog的三种格式对比

binlog内容分析

binlog格式对比

## 为什么会有mixed格式的binlog？

## 循环复制问题

双M结构

> 快速切换主从

通过serverid解决

为什么要有双m结构

原因

## 评论案例

部分从库数据异常

主库挂了

写压力大



# 25讲MySQL是怎么保证高可用的

https://time.geekbang.org/column/article/76795

MySQL高可用系统的基础，就是主备切换逻辑。

在满足数据可靠性的前提下，MySQL高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。

更建议使用可靠性优先的策略

## 主备延迟

同步延迟的判断 seconds_behind_master

网络传输和消费中转日志

一般大于1就不好 

## 主备延迟的来源

**机器性能差**

**备库读压力大**

**大事务**

> **一次性地用delete语句删除太多数据**
>
> **大表DDL**

**主库DML语句并发大,从库qps高，备库的并行复制能力**

主库和从库的参数配置不一样

从库上在进行备份操作

表上无主键的情况(主库利用索引更改数据,备库回放只能用全表扫描,这种情况可以调整slave_rows_search_algorithms参数适当优化下)

从库空间不足

## 可靠性优先策略

数据先同步到一致

## 可用性优先策略

可能出现数据不一致的情况

## **异常切换**

数据不一致前不能切换，消失的业务有可能重复做一次

大部分情况下只能等待数据恢复。

## 延迟45°逐渐上升

- 一种是大事务（包括大表DDL、一个事务操作很多行）；
- 还有一种情况比较隐蔽，就是备库起了一个长事务，然后主库加字段

# 26讲备库为什么会延迟好几个小时

https://time.geekbang.org/column/article/77083

MySQL的各种多线程复制策略。

slave_parallel_workers

## MySQL 5.5版本的并行复制策略

官方MySQL 5.5版本是不支持并行复制的。

### 按表分发策略

按表分发事务的基本思路是，如果两个事务更新不同的表，它们就可以并行

### 按行分发策略

基于行的策略，事务hash表中还需要考虑唯一键，即key应该是“库名+表名+索引a的名字+a的值”。

## MySQL 5.6版本的并行复制策略

按库并行

## MariaDB的并行复制策略

模拟主库的并行模式 

commit_id

## MySQL 5.7的并行复制策略

slave-parallel-type

> 1. 配置为DATABASE，表示使用MySQL 5.6版本的按库并行策略；
> 2. 配置为 LOGICAL_CLOCK，表示的就是类似MariaDB的策略。不过，MySQL 5.7这个策略，针对并行度做了优化。

并行复制策略的思想

> 1. 同时处于prepare状态的事务，在备库执行时是可以并行的；
> 2. 处于prepare状态的事务，与处于commit状态的事务之间，在备库执行时也是可以并行的。

## MySQL 5.7.22的并行复制策略

增加了一个新的并行复制策略，基于WRITESET的并行复制

binlog-transaction-dependency-tracking

没想到有什么场景必须得用WRITESET_SESSION

快速搭建从库用什么并行参数

# 27讲主库出问题了，从库怎么办

https://time.geekbang.org/column/article/77427

一主多从的切换正确性

## 基于位点的主备切换

change master命令

找同步位点并不精确且很复杂

跳过句子

> set global sql_slave_skip_counter=1;
>
> start slave;

跳过错误

> slave_skip_errors
>

## GTID

MySQL 5.6版本引入了GTID

```
GTID=server_uuid:gno
每次提交事务的时候分配给这个事务
```

transaction_id就是指事务id，事务id是在事务执行过程中分配的

## 基于GTID的主备切换

主备切换不是不需要找位点了，而是找位点这个工作，在实例A’内部就已经自动完成了。

## GTID和在线DDL

跳过事务

## gtid下binlog被删除

## 评论案例



# 28讲读写分离有哪些坑

一主多从架构的应用场景：读写分离，以及怎么处理主备延迟导致的读写分离问题。

客户端直连和带proxy的读写分离架构

**过期读**

先在客户端对请求做分类，区分哪些请求可以接受过期读，而哪些请求完全不能接受过期读；然后，对于不能接受过期读的语句，再使用等GTID或等位点的方案。

## 强制走主库方案

有时候你会碰到“所有查询都不能是过期读”的需求，比如一些金融类的业务。这样的话，你就要放弃读写分离，所有读写压力都在主库，等同于放弃了扩展性。



## Sleep 方案



## 判断主备无延迟方案

准确度确实提升了不少，但还是没有达到“精确”的程度。

## 配合semi-sync

## 等主库位点方案

```
select master_pos_wait(file, pos[, timeout]);
```

## 等待GTID方案

```
 select wait_for_executed_gtid_set(gtid_set, 1);
```



# 29讲如何判断一个数据库是不是出问题了

## select 1判断

innodb_thread_concurrency

**在线程进入锁等待以后，并发线程的计数会减一**

同时在执行的语句超过了设置的innodb_thread_concurrency的值，这时候系统其实已经不行了，但是通过select 1来检测系统，会认为系统还是正常的。

## 查表判断

health_check表

检测出由于并发线程过多导致的数据库不可用的情况

更新事务要写binlog，而一旦binlog所在磁盘的空间占用率达到100%，那么所有的更新语句和事务提交的commit语句就都会被堵住。但是，系统这时候还是可以正常读数据的。

## 更新判断

系统变慢

IO利用率100%

## 内部统计



## 评论

1.基础监控，包括硬盘，CPU，网络，内存等。
2.服务监控，包括jvm，服务端口，接入上下游服务的超时监控等。
3.业务监控，主要是监控业务的流程是否出现问题。



# 30讲答疑文章（二）：用动态的观点看加锁

https://time.geekbang.org/column/article/78427

## 不等号条件里的等值查询

id>9 and id<12 order by id desc

>  (0,5]、(5,10]和(10, 15)

id>10 and id<=15

> (10,15]

## 等值查询的过程

c in(5,20,10)

锁是“在执行过程中一个一个加的”，而不是一次性加上去的。

## 怎么看死锁？

解读LATESTDETECTED DEADLOCK

## 怎么看锁等待？

间隙蔓延

所谓“间隙”，其实根本就是由“这个间隙右边的那个记录”定义的。

## update的例子

间隙蔓延

## 空表也有间隙

 (-∞, supremum]



# 31讲误删数据后除了跑路，还能怎么办

## 误删行

Flashback工具

需要确保binlog_format=row 和 binlog_row_image=FULL。

临时备份库做操作

事前预防：

> sql_safe_updates
>
> 代码review

delete、truncate 、drop 

delete

> delete全表是很慢的，需要生成回滚日志、写redo、写binlog。所以，从性能角度考虑，你应该优先考虑使用truncate table或者drop table命令。
>
> binlog里面就只有一个truncate/drop 语句

## 误删库/表

全量备份，加增量日志

mysqlbinlog方法

主从方法

## 延迟复制备库

延迟复制的备库是一种特殊的备库，通过 CHANGE MASTER TO MASTER_DELAY = N命令，可以指定这个备库持续保持跟主库有N秒的延迟。

## 预防误删库/表的方法

账号分离，最低权限

制定操作规范

> 在删除数据表之前，必须先对表做改名操作

## rm删除数据

提前部署集群

# 32讲为什么还有kill不掉的语句

这些“kill不掉”的情况，其实是因为发送kill命令的客户端，并没有强行停止目标线程的执行，而只是设置了个状态，并唤醒对应的线程。而被kill的线程，需要执行到判断状态的“埋点”，才会开始进入终止逻辑阶段。并且，终止逻辑本身也是需要耗费时间的。

所以，如果你发现一个线程处于Killed状态，你可以做的事情就是，通过影响系统环境，让这个Killed状态尽快结束。

## 收到kill以后，线程做什么？

kill并不是马上停止的意思，而是告诉执行线程说，这条语句已经不需要继续执行了，可以开始“执行停止的逻辑了”

**kill query **

> 1. 把session B的运行状态改成THD::KILL_QUERY(将变量killed赋值为THD::KILL_QUERY)；
> 2. 给session B的执行线程发一个信号。

innodb_thread_concurrency满了进不去，**kill query**没有效果，要先进入innodb里面

在kill query无效的时候，其实kill connection也是无效的

**kill connection** 

> 1. 把12号线程状态设置为KILL_CONNECTION；
> 2. 关掉12号线程的网络连接。因为有这个操作，所以你会看到，这时候session C收到了断开连接的提示。

Command列显示为killed

> 如果一个线程的状态是KILL_CONNECTION，就把Command列显示成Killed。

kill无效 ，killed

> 1. **线程没有执行到判断线程状态的逻辑**
>
> 2. **终止逻辑耗时较长**

## 另外两个关于客户端的误解

**第一个误解是：如果库里面的表特别多，连接就会很慢。**

在连接命令中加上-A，就可以关掉这个自动补全的功能，然后客户端就可以快速返回了

**–quick是一个更容易引起误会的参数，是一个让服务端加速的参数。**

# 33讲我查这么多数据，会不会把数据库内存打爆

## 全表扫描对server层的影响

mysql取数据和发数据的流程

net_buffer

MySQL采用的是边算边发的逻辑。

仅当一个线程处于“等待客户端接收结果”的状态，才会显示"Sending to client"；而如果显示成“Sending data”，它的意思只是“正在执行”。

**对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，我都建议你使用mysql_store_result这个接口，直接把查询结果保存到本地内存。**

客户端如何接收数据，线程状态。



## 全表扫描对InnoDB的影响

Buffer Pool

内存命中率Buffer pool hit rate

Least Recently Used, LRU

按照5:3的比例把整个LRU链表分成了young区域和old区域

## 客户端迟迟不能接收结果

如果由于客户端压力太大，迟迟不能接收结果，会导致MySQL无法发送结果的后果。

如果第一个客户端执行select * from t for update 而迟迟不读取返回的数据，会造成server端长期占用记录的行锁，如果其他线程要更新被锁定的记录，会报锁等待超时的错误

# 34讲到底可不可以使用join

https://time.geekbang.org/column/article/79700

straight_join

## Index Nested-Loop Join（NLJ）

结论

1. 使用join语句，性能比强行拆成多个单表执行SQL语句的性能要好；
2. 如果使用join语句的话，需要让小表做驱动表。

## Simple Nested-Loop Join

t1每次到t2去匹配的时候，就要做一次全表扫描。

## Block Nested-Loop Join （BNL）

算法流程

分块去join

join_buffer是以无序数组的方式组织的，因此对表t2中的每一行，都要做100次判断

> 如果改用hash，会快很多

join_buffer_size

## 结论

第一个问题：能不能使用join语句？

1. 如果可以使用Index Nested-Loop Join算法，也就是说可以用上被驱动表上的索引，其实是没问题的；
2. 如果使用Block Nested-Loop Join算法，扫描行数就会过多。尤其是在大表上的join操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种join尽量不要用。

所以你在判断要不要使用join语句时，就是看explain结果里面，Extra字段里面有没有出现“Block Nested Loop”字样。

第二个问题是：如果要使用join，应该选择大表做驱动表还是选择小表做驱动表？

1. 如果是Index Nested-Loop Join算法，应该选择小表做驱动表；
2. 如果是Block Nested-Loop Join算法：
   - 在join_buffer_size足够大的时候，是一样的；
   - 在join_buffer_size不够大的时候（这种情况更常见），应该选择小表做驱动表。

所以，这个问题的结论就是，总是应该使用小表做驱动表。

## 什么是小表

**在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。**

# 35讲join语句怎么优化

## Multi-Range Read优化

read_rnd_buffer

如何稳定使用

Extra字段多了Using MRR

**MRR能够提升性能的核心**在于，这条查询语句在索引a上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。

## Batched Key Access

对NLJ算法的优化。

依赖于MRR

如何启动

join buffer充当read_rnd_buffer

如果被驱动表使用非递增主键(比如UUID作为主键),就没有必要开启MRR。

BKA算法是对NLJ算法的优化，一次取出一批数据的字段到join_buffer中，然后批量join，性能会比较好。BKA算法依赖于MRR，因为批量join找到被驱动表的非聚集索引字段通过MRR去查找行数据

## BNL算法的性能问题

被驱动表是一个大的冷数据表，除了会导致IO压力大以外，还会对系统有什么影响呢？

CPU资源

Buffer Pool的热数据被淘汰，新数据无法进入buffer

被驱动表大于或者小于old区域的时候的影响

## BNL转BKA

在被驱动表上建索引

筛选数据进入临时表，加索引

## 扩展-hash join

join buffer中采用hash

用业务完成

## 三个join的例子

评论区

select * ，所以放整行；你说得对，select * 不是好习惯；

# 36讲为什么临时表可以重名

用户临时表，与它相对应的，就是内部临时表，在[第17篇文章](https://time.geekbang.org/column/article/73795)中我已经和你介绍过。

## 内存表和临时表

内存表是Memory引擎的表

临时表是一种临时建立的表，不限引擎

## 临时表的特性

临时表只能被创建它的session访问

session结束的时候，会自动删除临时表

## 临时表的应用

由于不用担心线程之间的重名冲突，临时表经常会被用在复杂查询的优化过程中。

其中，分库分表系统的跨库查询就是一个典型的使用场景。

> 操作不能分库分表，需要在同一个表中操作，重新开发一个数据层太麻烦，把数据全部导入临时表再处理

## 为什么临时表可以重名？

物理存储

这个frm文件放在临时文件目录下，文件名的后缀是.frm，前缀是“#sql{进程id}_{线程id}_序列号”**

select @@tmpdir

内存存储

table_def_key

临时表链表

DROP TEMPORARY TABLE +表名

## 临时表和主备复制

临时表只在线程内自己可以访问，为什么需要写`DROP TEMPORARY TABLE`到binlog里面？

既然写binlog，就意味着备库需要。

如果当前的binlog_format=row，那么跟临时表有关的语句，就不会记录到binlog里。也就是说，只在binlog_format=statment/mixed 的时候，binlog中才会记录临时表的操作。

drop table在binlog中会被改写的原因

> 设置binlog_format=row时可能带了临时表，但是没有临时表

“/* generated by server */”

备库如何识别不同线程的同名临时表

> MySQL在记录binlog的时候，会把主库执行这个语句的线程id写到binlog中。

## 临时表改名

使用alter table语法修改临时表的表名，而不能使用rename语法。

> 在实现上，执行rename table语句的时候，要求按照“库名/表名.frm”的规则去磁盘找文件，但是临时表在磁盘上的frm文件是放在tmpdir目录下的，并且文件名的规则是“#sql{进程id}_{线程id}_序列号.frm”，因此会报“找不到文件名”的错误。

# 37讲什么时候会使用内部临时表

extra Using temporary

## union 执行流程

```mysql
(select 1000 as f) union (select id from t1 order by id desc limit 2);
```

新建临时表暂存数据，用上了临时表主键id的唯一性约束，实现了union的语义。

union改成union all的话，就没有了“去重”的语义，因此也就不需要临时表了。

## group by 执行流程

最后会做排序，可以加order by null跳过排序

一开始用内存临时表，太大会转为硬盘临时盘

> tmp_table_size，默认是16M

## group by 优化方法 --索引

group by逻辑都需要构造一个带唯一索引的表，执行代价都是比较高的。因为每一行都是无序的

如果扫描过程中可以保证出现的数据是有序的

计算group by的时候，就只需要从左到右，顺序扫描，依次累加。不需要临时表，也不需要再额外排序。

## group by优化方法 --直接排序

SQL_BIG_RESULT

直接用数组按顺序排好，然后按顺序数有几个

## distinct 和group by

如果只需要去重，不需要执行聚合函数，distinct 和group by那种效率高一些呢？

只需要去重的话，如果没有limit，是一样的；
有limit的话，distinct 快些。

# 38讲都说InnoDB好，那还要不要使用Memory引擎

## 内存表的数据组织结构

内存表的数据部分以数组的方式单独存放，而主键id索引里，存的是每个数据的位置。主键id是hash索引，索引上的key并不是有序的。

- InnoDB引擎把数据放在主键索引上，其他索引上保存的是主键id。这种方式，我们称之为**索引组织表**（Index Organizied Table）。
- 而Memory引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为**堆组织表**（Heap Organizied Table）。

**不同点**

## hash索引和B-Tree索引

不建议你在生产环境上使用内存表，除非临时表

内存表也是支B-Tree索引的。

## 内存表的锁

内存表不支持行锁，只支持表锁。

## 数据持久性问题

数据库重启的时候，所有的内存表都会被清空。同时删除主库的数据。

# 39讲自增主键为什么不是连续的

## 自增值保存在哪儿？

show create table

**表的结构定义存放在后缀名为.frm的文件中，但是并不会保存自增值。**

不同的引擎对于自增值的保存策略不同。

## 自增值修改机制

**新的自增值生成算法**

auto_increment_offset 和 auto_increment_increment

## 自增值的修改时机

在真正执行插入数据的操作之前

**导致自增主键id不连续**

> **唯一键冲突是导致自增主键id不连续的第一种原因。**
>
> 事务**回滚也会产生类似的现象**
>
> 大批量插入数据几何级申请

**自增值为什么不能回退。**

> 导致性能问题

## 自增锁的优化

### innodb_autoinc_lock_mode

优化加锁粒度

> 0. 等语句结束后才被释放，语句级别
>
> 1. 混合
>
>    > 原因：**批量插入**需要解决主从数据一致性
>
> 2. 申请完就马上释放
>
> 默认值是1。在8.0.3版本后，innodb_autoinc_lock_mode默认值已是2，在binlog_format默认值为row的前提下，想来也是为了增加并发。
>
> 从并发性能的角度考虑，我建议你将其设置为2，同时binlog_format设置为row。

### 批量申请自增id的策略

批量插入数据的语句，之所以需要这么设置，是因为“不知道要预先申请多少个id”

一个个申请比较慢

几何级别申请快

## 主从处理自增id

那么在binlog为statement的情况下。
语句A先获取id=1，然后B获取id=2，接着B提交，写binlog，再A写binlog。
这个时候如果binlog重放，是不是会发生B的id为1，而A的id为2的不一致的情况？

> 因为binlog在记录这种带自增值的语句之前，会在前面多一句，用于指定“接下来这个语句要需要的 自增ID值是多少”，而这个值，是在主库上这一行插入成功后对应的自增值，所以是一致的

## 插入后获取主键

一般都是insert执行完成后，再执行select last_insert_id

# 40讲insert语句的锁为什么这么多

## insert … select 语句

执行insert … select 的时候，对目标表也不是锁全表，而是只锁住需要访问的资源。

改良

> 用临时表

## insert 循环写入

读自己写入自己。

会导致在表t上做全表扫描，并且会给索引c上的所有间隙都加上共享的next-key lock。所以，这个语句执行期间，其他事务不能在这个表上插入数据。

原因

> 需要内部临时表
>
> 这类一边遍历数据，一边更新数据的情况，如果读出来的数据直接写回原表，就可能在遍历过程中，读到刚刚插入的记录，新插入的记录如果参与计算逻辑，就跟语义不符。

优化

> 引入用户临时表来做优化

## insert 唯一键冲突

insert 语句如果出现唯一键冲突，会在冲突的唯一值上加共享的next-key lock(S锁)。因此，碰到由于唯一键约束导致报错后，要尽快提交或回滚事务，避免加锁时间过长。

经典唯一键冲突导致的死锁场景

session还没有提交，但是这个记录已经作为最新记录写进去

优化

> 碰到由于唯一键约束导致报错后，要尽快提交或回滚事务，避免加锁时间过长。
>
> 并非只有insert,delete和update都可能造成死锁问题,核心还是插入唯一值冲突导致的.我们线上的处理办法是 1 去掉唯一值检测 2减少重复值的插入 3降低并发线程数量

## insert into … on duplicate key update

加入的是排他锁

**插入一行数据，如果碰到唯一键约束，就执行后面的更新语句。**

如果有多个列违反了唯一性约束，就会按照索引的顺序，修改跟第一个索引冲突的行。

affected rows返回的是2

> 真正更新的只有一行，只是在代码实现上，insert和update都认为自己成功了，update计数加了1， insert计数也加了1。

# 41讲怎么最快地复制一张表

## mysqldump方法

一条INSERT语句里面会包含多个value对

## 导出CSV文件

select … into outfile

## 物理拷贝方法

**MySQL 5.6版本可传输表空间**

# 42讲grant之后要跟着flushprivileges吗

## 创建用户

## 全局权限

全局权限，作用于整个MySQL实例，这些权限信息保存在mysql库的user表里。

1. grant 命令对于全局权限，同时更新了磁盘和内存。命令完成后即时生效，接下来新创建的连接会使用新的权限。
2. 对于一个已经存在的连接，它的全局权限不受grant命令的影响。

## db权限

为super是全局权限，这个权限信息在线程对象中，而revoke操作影响不到这个线程对象。

acl_dbs是一个全局数组，所有线程判断db权限都用这个数组，

如果当前会话已经处于某一个db里面，之前use这个库的时候拿到的库权限会保存在会话变量中。

## 表权限和列权限

## flush privileges使用场景

**因此，正常情况下，grant命令之后，没有必要跟着执行flush privileges命令。**

当数据表中的权限数据跟内存中的权限数据不一致的时候。

这种不一致往往是由不规范的操作导致的，比如直接用DML语句操作系统权限表。

# 43讲要不要使用分区表

## 分区表是什么？

每个分区对应一个.ibd文件。

## 分区表的引擎层行为

对于引擎层来说，不同分区不同的表不同的文件，索引组织分开

MyISAM的表锁是在引擎层实现的

## 分区表和手工分表

其实这两个方案的区别，主要是在server层上。

从引擎层看，这两种方式也是没有差别的。

## 分区策略

每当第一次访问一个分区表的时候，MySQL需要把所有的分区都访问一遍。

MyISAM分区表使用的分区策略，我们称为**通用分区策略**。

从MySQL 5.7.9开始，InnoDB引擎引入了**本地分区策略**。

从MySQL 8.0版本开始，就不允许创建MyISAM分区表了，只允许创建已经实现了本地分区策略的引擎。目前来看，只有InnoDB和NDB这两个引擎支持了本地分区策略。

## 分区表的server层行为

如果从server层看的话，一个分区表就只是一个表。

打开文件

> “本地分区规则应该是只打开单个引擎文件”，并不是哈，我在文章末尾说了，也会打开所有文件的，只是说本地分区规则有优化，比如如果文件数过多，就会淘汰之前打开的文件句柄（暂时关掉）。
>
>   在InnoDB引擎打开文件超过 innodb_open_files这个值的时候，就会关掉一些之前打开的文件。
>
> 其实我们文章中 ，InnoDB分区表使用了本地分区策略以后，即使分区个数大于open_files_limit ，打开InnoDB分区表也不会报“打开文件过多”这个错误，就是innodb_open_files这个参数发挥的作用。  

## 总结

1. MySQL在第一次打开分区表的时候，需要访问所有的分区；
2. 在server层，认为这是同一张表，因此所有分区共用同一个MDL锁；分区表，在做DDL的时候，影响会更大。
3. 在引擎层，认为这是不同的表，因此MDL锁之后的执行过程，会根据分区表规则，只访问必要的分区。

## 分区表的应用场景

注意点

> 分区并不是越细越好。
>
> 分区也不要提前预留太多，在使用之前预先创建即可。

## 分区表的自增主键

虽然分区了，但是分区里面如果没有分区键索引，还是无序的。

分区表中的主键必须包含分区字段。

InnoDB表要求至少有一个索引，以自增字段作为第一个字段。

如果从利用率上来看，应该使用(ftime, id)这种模式。因为用ftime做分区key，说明大多数语句都是包含ftime的，使用这种模式，可以利用前缀索引的规则，减少一个索引。

# 44讲答疑文章（三）：说一说这些好问题

## join的写法

有可能语句虽然用的是left join，但是语义跟join是一致的，会被优化。

在MySQL里，NULL跟任何值执行等值判断和不等值判断的结果，都是NULL。这里包括， select NULL = NULL 的结果，也是返回NULL。

**如果需要left join的语义，就不能把被驱动表的字段放在where条件里面做等值判断或不等值判断，必须都写在on里面。**

## Simple Nested Loop Join 的性能问题

Simple Nested Loop Join要比bnl差

## distinct 和 group by的性能

```
select a from t group by a order by null;
select distinct a from t;
```

标准的group by语句，是需要在select部分加一个聚合函数

如果只需要去重，不需要执行聚合函数，那么distinct 和group by这两条语句的语义和执行流程是相同的，因此执行性能也相同。

## 备库自增主键问题

statement，binlog会有SET INSERT_ID。